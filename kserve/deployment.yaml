apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
  name: "llmserving"
spec:
  predictor:
    scaleTarget: 2
    scaleMetric: concurrency
    containerConcurrency: 1
    containers:
    - image: leyao/llmserving:v0.2
      imagePullPolicy: IfNotPresent
      resources:
        requests:
          memory: "30Gi"
          cpu: "3"
        limits:
          memory: "30Gi"
          cpu: "3"
      
